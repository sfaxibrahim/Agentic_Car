{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f97328c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage , AIMessage\n",
    "import json \n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List , Dict, Any\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3e515aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a98265c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG={\n",
    "    \"model_name\":\"mistral:latest\",\n",
    "    \"memory_window\":5,\n",
    "    \"history_file_path\":\"chat_history.json\",\n",
    "    \"max_history_entries\":100,\n",
    "    \"save_history\":True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c09c3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    \n",
    "    llm = OllamaLLM(model=CONFIG[\"model_name\"], callbacks=[StreamingStdOutCallbackHandler()], verbose=True)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7312e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_pormpt():\n",
    "    \"\"\"create a prompt template with specific instructions for the AI model.\"\"\"\n",
    "    template = \"\"\"You are a friendly, intelligent AI assistant having a natural conversation.\n",
    "\n",
    "    Guidelines:\n",
    "    - Be conversational and engaging, not robotic or formal\n",
    "    - Ask thoughtful follow-up questions when appropriate\n",
    "    - Show genuine curiosity about the user's interests\n",
    "    - Reference relevant parts of our conversation history\n",
    "    - Provide helpful, accurate information\n",
    "    - Admit when you don't know something\n",
    "    - Keep responses concise but informative\n",
    "\n",
    "    Recent conversation history (last {memory_window} exchanges):\n",
    "    {history}\n",
    "\n",
    "    Current exchange:\n",
    "    Human: {input}\n",
    "    AI: \"\"\"\n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"history\", \"input\", \"memory_window\"],\n",
    "        template=template,\n",
    "        partial_variables={\"memory_window\": CONFIG[\"memory_window\"]}\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f0d1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_memory_system(llm):\n",
    "    \"\"\"setup the memory system for the conversation chain\"\"\"\n",
    "    window_memory = ConversationBufferWindowMemory(\n",
    "        k=CONFIG[\"memory_window\"],\n",
    "        return_messages=True,\n",
    "        memory_key=\"history\",\n",
    "        ai_prefix=\"AI\",\n",
    "        human_prefix=\"Human\"\n",
    "    )\n",
    "    # Secondary memory: summarizes older conversations\n",
    "    summary_memory = ConversationSummaryBufferMemory(\n",
    "            llm=llm,\n",
    "            max_token_limit=1500,\n",
    "            return_messages=True,\n",
    "            memory_key=\"summary\",\n",
    "            human_prefix=\"Human\",\n",
    "            ai_prefix=\"AI\"\n",
    "        )    \n",
    "    return window_memory , summary_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2d8dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chat_history(memory):\n",
    "    \"\"\"load conversation history from file and popualte memory\"\"\"\n",
    "\n",
    "    if not CONFIG[\"save_history\"] or not os.path.exists(CONFIG[\"history_file_path\"]):\n",
    "        return 0\n",
    "    try:\n",
    "        with open(CONFIG[\"history_file_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            history_data = json.load(f)\n",
    "        \n",
    "        recent_exchanges=history_data.get('exchanges' , [])[-CONFIG[\"memory_window\"]:]\n",
    "\n",
    "        for exchange in recent_exchanges:\n",
    "            memory.chat_memory.add_user_message(exchange['human'])\n",
    "            memory.chat_memory.add_ai_message(exchange['ai'])\n",
    "        \n",
    "        logger.info(f\"Loaded {len(recent_exchanges)} exchanges from history.\")\n",
    "        return len(recent_exchanges)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading chat history: {e}\")\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "538a91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_exchange_to_history(human_input: str, ai_response: str):\n",
    "    \"\"\"Save individual exchange to persistent history\"\"\"\n",
    "    if not CONFIG[\"save_history\"]:\n",
    "        return\n",
    "    \n",
    "    exchange = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"human\": human_input,\n",
    "        \"ai\": ai_response\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load existing history or create new\n",
    "        if os.path.exists(CONFIG[\"history_file\"]):\n",
    "            with open(CONFIG[\"history_file\"], 'r', encoding='utf-8') as f:\n",
    "                history_data = json.load(f)\n",
    "        else:\n",
    "            history_data = {\"exchanges\": [], \"metadata\": {}}\n",
    "        \n",
    "        # Add new exchange\n",
    "        history_data[\"exchanges\"].append(exchange)\n",
    "        \n",
    "        # Keep only recent entries to prevent file bloat\n",
    "        if len(history_data[\"exchanges\"]) > CONFIG[\"max_history_entries\"]:\n",
    "            history_data[\"exchanges\"] = history_data[\"exchanges\"][-CONFIG[\"max_history_entries\"]:]\n",
    "        \n",
    "        # Update metadata\n",
    "        history_data[\"metadata\"] = {\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"total_exchanges\": len(history_data[\"exchanges\"]),\n",
    "            \"memory_window\": CONFIG[\"memory_window\"]\n",
    "        }\n",
    "        \n",
    "        # Save back to file\n",
    "        with open(CONFIG[\"history_file\"], 'w', encoding='utf-8') as f:\n",
    "            json.dump(history_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not save exchange: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "824670c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_stats(memory) -> Dict[str, Any]:\n",
    "    \"\"\"Get statistics about the current conversation\"\"\"\n",
    "    messages = memory.chat_memory.messages\n",
    "    return {\n",
    "        \"exchanges_in_memory\": len(messages) // 2,\n",
    "        \"total_tokens_approx\": sum(len(msg.content.split()) for msg in messages),\n",
    "        \"memory_window\": CONFIG[\"memory_window\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e12b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_conversation_summary(memory,summary_memory):\n",
    "    \"\"\"Display a summary of the conversation so far\"\"\"\n",
    "    stats=get_conversation_stats(memory)\n",
    "    print(f\"\\n--- Conversation Summary ---\")\n",
    "    print(f\"Exchanges in Memory: {stats['exchanges_in_memory']}\")\n",
    "    print(f\"Approx. Tokens in Memory: {stats['total_tokens_approx']}\")\n",
    "    print(f\"Memory window: {stats['memory_window']} exchanges\")\n",
    "\n",
    "    # Try to get conversation summary\n",
    "    try:\n",
    "        if hasattr(summary_memory, 'predict_new_summary') and memory.chat_memory.messages:\n",
    "            summary = summary_memory.predict_new_summary(memory.chat_memory.messages, \"\")\n",
    "            print(f\"Conversation themes: {summary[:200]}...\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not generate summary: {e}\")\n",
    "        \n",
    "    print(\"--- End Summary ---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1bac5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_commands(user_input: str, memory, summary_memory) -> bool:\n",
    "    \"\"\"Handle special commands like /help, /stats, /clear, etc.\"\"\"\n",
    "    user_input = user_input.strip().lower()\n",
    "    \n",
    "    if user_input in ['/help', '/h']:\n",
    "        print(\"\"\"\n",
    "Available commands:\n",
    "/help, /h          - Show this help\n",
    "/stats, /s         - Show conversation statistics\n",
    "/summary           - Show conversation summary\n",
    "/clear             - Clear conversation memory\n",
    "/history           - Show recent exchange history\n",
    "/config            - Show current configuration\n",
    "/exit, /quit, /bye - Exit the chat\n",
    "        \"\"\")\n",
    "        return True\n",
    "    \n",
    "    elif user_input in ['/stats', '/s']:\n",
    "        stats = get_conversation_stats(memory)\n",
    "        print(f\"\\nConversation Statistics:\")\n",
    "        print(f\"- Exchanges in memory: {stats['exchanges_in_memory']}\")\n",
    "        print(f\"- Approximate tokens: {stats['total_tokens_approx']}\")\n",
    "        print(f\"- Memory window: {stats['memory_window']}\")\n",
    "        return True\n",
    "    \n",
    "    elif user_input == '/summary':\n",
    "        display_conversation_summary(memory, summary_memory)\n",
    "        return True\n",
    "    \n",
    "    elif user_input == '/clear':\n",
    "        memory.clear()\n",
    "        print(\"Conversation memory cleared!\")\n",
    "        return True\n",
    "    \n",
    "    elif user_input == '/history':\n",
    "        messages = memory.chat_memory.messages\n",
    "        print(f\"\\nRecent conversation history ({len(messages)//2} exchanges):\")\n",
    "        for i in range(0, len(messages), 2):\n",
    "            if i+1 < len(messages):\n",
    "                print(f\"Human: {messages[i].content}\")\n",
    "                print(f\"AI: {messages[i+1].content[:100]}...\")\n",
    "                print(\"-\" * 40)\n",
    "        return True\n",
    "    \n",
    "    elif user_input == '/config':\n",
    "        print(f\"\\nCurrent Configuration:\")\n",
    "        for key, value in CONFIG.items():\n",
    "            print(f\"- {key}: {value}\")\n",
    "        return True\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2034bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_input(user_input: str) -> tuple[bool, str]:\n",
    "    \"\"\"Validate and clean user input\"\"\"\n",
    "    if not user_input or user_input.isspace():\n",
    "        return False, \"Please enter a valid message.\"\n",
    "    \n",
    "    # Clean input\n",
    "    cleaned_input = user_input.strip()\n",
    "    \n",
    "    # Check for extremely long input\n",
    "    if len(cleaned_input) > 2000:\n",
    "        return False, \"Message too long. Please keep it under 2000 characters.\"\n",
    "    \n",
    "    return True, cleaned_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce895bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_robust_chatbot():\n",
    "    \"\"\"Create and return a fully configured chatbot with enhanced features\"\"\"\n",
    "    print(\"ðŸ¤– Initializing Enhanced Conversational AI Pipeline...\")\n",
    "    \n",
    "    # Initialize components\n",
    "    llm = initialize_llm()\n",
    "    prompt = create_enhanced_pormpt()\n",
    "    window_memory, summary_memory = setup_memory_system(llm)\n",
    "    \n",
    "    # Load previous history\n",
    "    loaded_exchanges = load_chat_history(window_memory)\n",
    "    if loaded_exchanges > 0:\n",
    "        print(f\"ðŸ“š Loaded {loaded_exchanges} previous exchanges from history\")\n",
    "    \n",
    "    # Create conversation chain\n",
    "    chatbot = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=window_memory,\n",
    "        prompt=prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return chatbot, window_memory, summary_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bfe3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation_loop():\n",
    "    \"\"\"Main conversation loop with enhanced error handling and features\"\"\"\n",
    "    print(\"ðŸš€ Starting Enhanced Conversational AI\")\n",
    "    print(\"Type '/help' for commands or '/exit' to quit\\n\")\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    chatbot, memory, summary_memory = create_robust_chatbot()\n",
    "    \n",
    "    conversation_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                # Get user input\n",
    "                user_input = input(\"\\nðŸ§‘ You: \").strip()\n",
    "                \n",
    "                # Handle exit conditions\n",
    "                if user_input.lower() in ['exit', 'quit', 'bye', '/exit', '/quit', '/bye']:\n",
    "                    print(\"\\nðŸ‘‹ Thanks for chatting! Goodbye!\")\n",
    "                    display_conversation_summary(memory, summary_memory)\n",
    "                    break\n",
    "                \n",
    "                # Handle special commands\n",
    "                if handle_special_commands(user_input, memory, summary_memory):\n",
    "                    continue\n",
    "                \n",
    "                # Validate input\n",
    "                is_valid, processed_input = validate_input(user_input)\n",
    "                if not is_valid:\n",
    "                    print(f\"âš ï¸  {processed_input}\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate response\n",
    "                print(\"\\nðŸ¤– AI: \", end=\"\", flush=True)\n",
    "                try:\n",
    "                    response = chatbot.predict(input=processed_input)\n",
    "                    print()  # New line after streaming response\n",
    "                    \n",
    "                    # Save to persistent history\n",
    "                    save_exchange_to_history(processed_input, response)\n",
    "                    \n",
    "                    # Update conversation stats\n",
    "                    conversation_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating response: {e}\")\n",
    "                    print(f\"\\nâŒ Sorry, I encountered an error: {e}\")\n",
    "                    print(\"Please try rephrasing your message.\")\n",
    "                    continue\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nâ¸ï¸  Conversation paused. Type '/exit' to quit or continue chatting.\")\n",
    "                continue\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error in conversation loop: {e}\")\n",
    "                print(f\"\\nâŒ An unexpected error occurred: {e}\")\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in conversation loop: {e}\")\n",
    "        print(f\"\\nðŸ’¥ Fatal error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        print(f\"\\nðŸ“Š Session ended. Total exchanges: {conversation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70ed5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Enhanced Conversational AI\n",
      "Type '/help' for commands or '/exit' to quit\n",
      "\n",
      "ðŸ¤– Initializing Enhanced Conversational AI Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrah\\AppData\\Local\\Temp\\ipykernel_4104\\2783214871.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  window_memory = ConversationBufferWindowMemory(\n",
      "C:\\Users\\ibrah\\AppData\\Local\\Temp\\ipykernel_4104\\2783214871.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– AI: \n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly, intelligent AI assistant having a natural conversation.\n",
      "\n",
      "    Guidelines:\n",
      "    - Be conversational and engaging, not robotic or formal\n",
      "    - Ask thoughtful follow-up questions when appropriate\n",
      "    - Show genuine curiosity about the user's interests\n",
      "    - Reference relevant parts of our conversation history\n",
      "    - Provide helpful, accurate information\n",
      "    - Admit when you don't know something\n",
      "    - Keep responses concise but informative\n",
      "\n",
      "    Recent conversation history (last 5 exchanges):\n",
      "    []\n",
      "\n",
      "    Current exchange:\n",
      "    Human: hello i am steve 25 year old\n",
      "    AI: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Steve! It's nice to meet you. You seem to be in your mid-twenties, correct? How can I assist you today or help make your day a bit more enjoyable? Do you have any specific interests or questions you'd like to discuss? I'm always here to chat about various topics and help you find the information you need!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Could not save exchange: 'history_file'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ¤– AI: \n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly, intelligent AI assistant having a natural conversation.\n",
      "\n",
      "    Guidelines:\n",
      "    - Be conversational and engaging, not robotic or formal\n",
      "    - Ask thoughtful follow-up questions when appropriate\n",
      "    - Show genuine curiosity about the user's interests\n",
      "    - Reference relevant parts of our conversation history\n",
      "    - Provide helpful, accurate information\n",
      "    - Admit when you don't know something\n",
      "    - Keep responses concise but informative\n",
      "\n",
      "    Recent conversation history (last 5 exchanges):\n",
      "    [HumanMessage(content='hello i am steve 25 year old', additional_kwargs={}, response_metadata={}), AIMessage(content=\" Hello Steve! It's nice to meet you. You seem to be in your mid-twenties, correct? How can I assist you today or help make your day a bit more enjoyable? Do you have any specific interests or questions you'd like to discuss? I'm always here to chat about various topics and help you find the information you need!\", additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "    Current exchange:\n",
      "    Human: who am i.\n",
      "    AI: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh, I apologize for any confusion! It seems we are still getting to know each other. As you introduced yourself earlier as Steve, that's the name I'm currently using. If I've made a mistake or if there's something else you'd prefer me to call you, feel free to let me know!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Could not save exchange: 'history_file'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ‘‹ Thanks for chatting! Goodbye!\n",
      "\n",
      "--- Conversation Summary ---\n",
      "Exchanges in Memory: 2\n",
      "Approx. Tokens in Memory: 119\n",
      "Memory window: 5 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The human introduces themselves as Steve, a 25-year-old individual. The AI greets Steve and offers assistance or conversation on various topics. When asked about his identity, the AI clarifies that it's using the name 'Steve' based on the introduction, but is open to changes if preferred.Conversation themes:  The human introduces themselves as Steve, a 25-year-old individual. The AI greets Steve and offers assistance or conversation on various topics. When asked about his identity, the AI clarifies that i...\n",
      "--- End Summary ---\n",
      "\n",
      "\n",
      "ðŸ“Š Session ended. Total exchanges: 2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        run_conversation_loop()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nðŸ‘‹ Goodbye!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9336a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
